{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6tasuafvT2O"
   },
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvJBeHv3vfE_"
   },
   "source": [
    "Deep Video Detection using CNN's and RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyukrIAev64Y"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZsX2Wrrv9Op"
   },
   "source": [
    "DeepFake is composed from Deep Learning and Fake and means taking one person from an image or video and replacing with someone else likeness using technology such as Deep Artificial Neural Networks. Large companies like Google invest very much in fighting the DeepFake, this including release of large datasets to help training models to counter this threat.The phenomen invades rapidly the film industry and threatens to compromise news agencies. Large digital companies, including content providers and social platforms are in the frontrun of fighting Deep Fakes. GANs that generate DeepFakes becomes better every day and, of course, if you include in a new GAN model all the information we collected until now how to combat various existent models, we create a model that cannot be beatten by the existing ones.\n",
    "\n",
    "First we will work on detecting faces that were forged and we will work on developing a model to detect videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Fake Video Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files\n",
    "\n",
    "* train_sample_videos.zip - a ZIP file containing a sample set of training videos and a metadata.json with labels. the full set of training videos is available through the links provided above.\n",
    "* sample_submission.csv - a sample submission file in the correct format.\n",
    "* test_videos.zip - a zip file containing a small set of videos to be used as a public validation set. To understand the datasets available for this competition, review the Getting Started information.\n",
    "\n",
    "Metadata Columns\n",
    "\n",
    "* filename - the filename of the video\n",
    "* label - whether the video is REAL or FAKE\n",
    "* original - in the case that a train set video is FAKE, the original video is listed here\n",
    "* split - this is always equal to \"train\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow-docs==2024.2.5.73858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "#from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../input/deepfake-detection-challenge'\n",
    "TRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n",
    "TEST_FOLDER = 'test_videos'\n",
    "\n",
    "print(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\n",
    "print(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n",
    "train_sample_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize now the data.\n",
    "\n",
    "We select first a list of fake videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few fake videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].sample(10).index)\n",
    "fake_train_sample_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_video(video_path):\n",
    "    '''\n",
    "    input: video_path - path for video\n",
    "    process:\n",
    "    1. perform a video capture from the video\n",
    "    2. read the image\n",
    "    3. display the image\n",
    "    '''\n",
    "    capture_image = cv2.VideoCapture(video_path) \n",
    "    ret, frame = capture_image.read()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in fake_train_sample_video:\n",
    "    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now the same for few of the images that are real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Real Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='REAL'].sample(5).index)\n",
    "real_train_sample_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in real_train_sample_video:\n",
    "    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos with same original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look now to set of samples with the same original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata['original'].value_counts()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick one of the originals with largest number of samples.\n",
    "\n",
    "We also modify our visualization function to work with multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_video_list(video_path_list, video_folder=TRAIN_SAMPLE_FOLDER):\n",
    "    '''\n",
    "    input: video_path_list - path for video\n",
    "    process:\n",
    "    0. for each video in the video path list\n",
    "        1. perform a video capture from the video\n",
    "        2. read the image\n",
    "        3. display the image\n",
    "    '''\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2,3,figsize=(16,8))\n",
    "    # we only show images extracted from the first 6 videos\n",
    "    for i, video_file in enumerate(video_path_list[0:6]):\n",
    "        video_path = os.path.join(DATA_FOLDER, video_folder,video_file)\n",
    "        capture_image = cv2.VideoCapture(video_path) \n",
    "        ret, frame = capture_image.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        ax[i//3, i%3].imshow(frame)\n",
    "        ax[i//3, i%3].set_title(f\"Video: {video_file}\")\n",
    "        ax[i//3, i%3].axis('on')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_original_fake_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.original=='atvmxvwyns.mp4'].index)\n",
    "display_image_from_video_list(same_original_fake_train_sample_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test video files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look to few of the test data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_videos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize now one of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_video(os.path.join(DATA_FOLDER, TEST_FOLDER, test_videos.iloc[2].video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play video files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to few fake videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_videos = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n",
    "    '''\n",
    "    Display video\n",
    "    param: video_file - the name of the video file to display\n",
    "    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)\n",
    "    '''\n",
    "    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n",
    "    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)\n",
    "\n",
    "play_video(fake_videos[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visual inspection of these fakes videos, in some cases is very easy to spot the anomalies created when engineering the deep fake, in some cases is more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A CNN-RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this example we will do the following:\n",
    "\n",
    "* Capture the frames of a video.\n",
    "* Extract frames from the videos until a maximum frame count is reached.\n",
    "* In the case, where a video's frame count is lesser than the maximum frame count we will pad the video with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a pre-trained network to extract meaningful features from the extracted frames. The Keras Applications module provides a number of state-of-the-art models pre-trained on the ImageNet-1k dataset. We will be using the InceptionV3 model for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put all the pieces together to create our data processing utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = list(df.index)\n",
    "    labels = df[\"label\"].values\n",
    "    labels = np.array(labels=='FAKE').astype(np.int)\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have test labels we split the training data to find its performance in unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_set, Test_set = train_test_split(train_sample_metadata,test_size=0.2,random_state=42,stratify=train_sample_metadata['label'])\n",
    "\n",
    "print(Train_set.shape, Test_set.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = prepare_all_videos(Train_set, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(Test_set, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can feed this data to a sequence model consisting of recurrent layers like GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "# mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "# # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "# # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "# x = keras.layers.GRU(16, return_sequences=True)(\n",
    "#     frame_features_input, mask=mask_input\n",
    "# )\n",
    "# x = keras.layers.GRU(8)(x)\n",
    "# x = keras.layers.Dropout(0.4)(x)\n",
    "# x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "# output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = keras.callbacks.ModelCheckpoint('./', save_weights_only=True, save_best_only=True)\n",
    "# history = model.fit(\n",
    "#         [train_data[0], train_data[1]],\n",
    "#         train_labels,\n",
    "#         validation_data=([test_data[0], test_data[1]],test_labels),\n",
    "#         callbacks=[checkpoint],\n",
    "#         epochs=EPOCHS,\n",
    "#         batch_size=BATCH_SIZE\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "x = keras.layers.GRU(64, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.GRU(32, return_sequences=True)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.GRU(16)(x)\n",
    "x = keras.layers.Dropout(0.5)(x)  # Increased dropout\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)  # Added another dropout layer\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "# Use a lower learning rate and different optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Include Early Stopping to prevent overfitting\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('./', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [train_data[0], train_data[1]],\n",
    "    train_labels,\n",
    "    validation_data=([test_data[0], test_data[1]], test_labels),\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the data from the history object\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, accuracy, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'ro-', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER,path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    return model.predict([frame_features, frame_mask])[0]\n",
    "    \n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "    return embed.embed_file(\"animation.gif\")\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_videos[\"video\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "if(sequence_prediction(test_video)>=0.5):\n",
    "    print(f'The predicted class of the video is FAKE')\n",
    "else:\n",
    "    print(f'The predicted class of the video is REAL')\n",
    "\n",
    "play_video(test_video,TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('deepfake_video_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import imageio\n",
    "# import cv2\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# DATA_FOLDER = '../input/deepfake-detection-challenge'\n",
    "# TRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n",
    "# TEST_FOLDER = 'test_videos'\n",
    "\n",
    "# IMG_SIZE = 224\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 30\n",
    "# MAX_SEQ_LENGTH = 20\n",
    "# NUM_FEATURES = 2048\n",
    "\n",
    "# def crop_center_square(frame):\n",
    "#     y, x = frame.shape[0:2]\n",
    "#     min_dim = min(y, x)\n",
    "#     start_x = (x // 2) - (min_dim // 2)\n",
    "#     start_y = (y // 2) - (min_dim // 2)\n",
    "#     return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "# def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "#     cap = cv2.VideoCapture(path)\n",
    "#     frames = []\n",
    "#     try:\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame = crop_center_square(frame)\n",
    "#             frame = cv2.resize(frame, resize)\n",
    "#             frame = frame[:, :, [2, 1, 0]]\n",
    "#             frames.append(frame)\n",
    "#             if len(frames) == max_frames:\n",
    "#                 break\n",
    "#     finally:\n",
    "#         cap.release()\n",
    "#     return np.array(frames)\n",
    "\n",
    "# def build_feature_extractor():\n",
    "#     feature_extractor = keras.applications.InceptionV3(\n",
    "#         weights=\"imagenet\",\n",
    "#         include_top=False,\n",
    "#         pooling=\"avg\",\n",
    "#         input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     )\n",
    "#     preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "#     inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "#     preprocessed = preprocess_input(inputs)\n",
    "#     outputs = feature_extractor(preprocessed)\n",
    "#     return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "# feature_extractor = build_feature_extractor()\n",
    "\n",
    "# def prepare_all_videos(df, root_dir):\n",
    "#     num_samples = len(df)\n",
    "#     video_paths = list(df.index)\n",
    "#     labels = df[\"label\"].values\n",
    "#     labels = np.array(labels == 'FAKE').astype(np.int)\n",
    "\n",
    "#     frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "#     frame_features = np.zeros(\n",
    "#         shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "#     )\n",
    "\n",
    "#     for idx, path in enumerate(video_paths):\n",
    "#         frames = load_video(os.path.join(root_dir, path))\n",
    "#         frames = frames[None, ...]\n",
    "\n",
    "#         temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "#         temp_frame_features = np.zeros(\n",
    "#             shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "#         )\n",
    "\n",
    "#         for i, batch in enumerate(frames):\n",
    "#             video_length = batch.shape[0]\n",
    "#             length = min(MAX_SEQ_LENGTH, video_length)\n",
    "#             for j in range(length):\n",
    "#                 temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "#                     batch[None, j, :]\n",
    "#                 )\n",
    "#             temp_frame_mask[i, :length] = 1\n",
    "\n",
    "#         frame_features[idx, ] = temp_frame_features.squeeze()\n",
    "#         frame_masks[idx, ] = temp_frame_mask.squeeze()\n",
    "\n",
    "#     return (frame_features, frame_masks), labels\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train_set, Test_set = train_test_split(train_sample_metadata, test_size=0.2, random_state=42, stratify=train_sample_metadata['label'])\n",
    "\n",
    "# train_data, train_labels = prepare_all_videos(Train_set, TRAIN_SAMPLE_FOLDER)\n",
    "# test_data, test_labels = prepare_all_videos(Test_set, TRAIN_SAMPLE_FOLDER)\n",
    "\n",
    "# frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "# mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "# x = keras.layers.GRU(64, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.GRU(32, return_sequences=True)(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.GRU(16)(x)\n",
    "# x = keras.layers.Dropout(0.5)(x)  # Increased dropout\n",
    "# x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.5)(x)  # Added another dropout layer\n",
    "# output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "# # Use a lower learning rate and different optimizer\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# model.summary()\n",
    "\n",
    "# # Include Early Stopping to prevent overfitting\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint('./', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# history = model.fit(\n",
    "#     [train_data[0], train_data[1]],\n",
    "#     train_labels,\n",
    "#     validation_data=([test_data[0], test_data[1]], test_labels),\n",
    "#     callbacks=[checkpoint, early_stopping],\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=16\n",
    "# )\n",
    "# def prepare_single_video(frames):\n",
    "#     frames = frames[None, ...]\n",
    "#     frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "#     frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "#     for i, batch in enumerate(frames):\n",
    "#         video_length = batch.shape[0]\n",
    "#         length = min(MAX_SEQ_LENGTH, video_length)\n",
    "#         for j in range(length):\n",
    "#             frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "#         frame_mask[i, :length] = 1\n",
    "\n",
    "#     return frame_features, frame_mask\n",
    "\n",
    "# def sequence_prediction(path):\n",
    "#     frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER, path))\n",
    "#     frame_features, frame_mask = prepare_single_video(frames)\n",
    "#     return model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "# test_video = np.random.choice(test_videos[\"video\"].values.tolist())\n",
    "# print(f\"Test video path: {test_video}\")\n",
    "\n",
    "# if sequence_prediction(test_video) >= 0.5:\n",
    "#     print(f'The predicted class of the video is FAKE')\n",
    "# else:\n",
    "#     print(f'The predicted class of the video is REAL')\n",
    "\n",
    "# play_video(test_video, TEST_FOLDER)\n",
    "\n",
    "# model.save('deepfake_video_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used simple RNN model feel free to try some complex Attention based and Transformer based models"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 858837,
     "sourceId": 16880,
     "sourceType": "competition"
    },
    {
     "datasetId": 464091,
     "sourceId": 924245,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29844,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
